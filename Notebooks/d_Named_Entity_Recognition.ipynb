{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Перед началом работы"
      ],
      "metadata": {
        "id": "b93_NYDDuDI9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Примапим диск"
      ],
      "metadata": {
        "id": "N03qOq9fudPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "xeh6h_AZuU8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Устанавливаем зависимости"
      ],
      "metadata": {
        "id": "QaNVvuSTuqVk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "repo_folder = '/content/drive/MyDrive/DeepLearning/'"
      ],
      "metadata": {
        "id": "CVOkho41FAsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JyfPU-UK22bQ"
      },
      "outputs": [],
      "source": [
        "reqs_path = repo_folder + 'IntelligentDocumentProcessing/requirements.txt '\n",
        "!pip3 install -r {reqs_path}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Проверим корректность установки библиотек после перезагрузки ядра"
      ],
      "metadata": {
        "id": "NG5kkm1hEjre"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGYIDT3JdyGh"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "# Библиотеки для нейронных сетей\n",
        "import torch                                                            # The one and the only\n",
        "import pytorch_lightning as pl                                          # Циклы обучения и инструменты\n",
        "from sklearn.metrics import f1_score, classification_report             # Метрики для задачи\n",
        "from torch.utils.data import Dataset, DataLoader, random_split          # Утилиты для работы с данными PyTorch\n",
        "from transformers import BertTokenizer, BertForTokenClassification      # Библиотека, где можно разжиться предобученными трансформерными моделями\n",
        "\n",
        "\n",
        "## Сериализация и коллекции\n",
        "\n",
        "import json\n",
        "import collections\n",
        "from typing import List, Tuple, Callable, Dict, Set, Union\n",
        "from glob import glob\n",
        "\n",
        "\n",
        "## NLP инструменты (разбиение текста на нормализованные слова)\n",
        "from rusenttokenize import ru_sent_tokenize\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "\n",
        "\n",
        "## Отрисовка и представление\n",
        "import seaborn as sns\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image\n",
        "from ipymarkup import show_span_line_markup\n",
        "\n",
        "sns.set_theme(style=\"white\", context=\"talk\")\n",
        "plt.style.use('default')\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Добавим путь запуска"
      ],
      "metadata": {
        "id": "EloGDr9xE1KP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_folder = repo_folder + 'IntelligentDocumentProcessing/Resources/4_Named_Entity_Recognition/'\n",
        "sys.path.append(base_folder)"
      ],
      "metadata": {
        "id": "E3pTxU5KE3S9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0koq6963n8x"
      },
      "source": [
        "# 0. Введение"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4twyE2Q3wvSj"
      },
      "source": [
        "## 0.1. Распознавание именованных сущностей"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6DBgArHyI8y"
      },
      "source": [
        "\n",
        "Мы нашли текст на документе, распознали символы, из которых текст состоит и связали символы в слова и предложения.\n",
        "\n",
        "В нашем проекте нам необходимо над текстом сделать некоторую аналитику: распознать названия и имена в тексте (в нашей задаче будем распознавать только имена и названия организаций).\n",
        "\n",
        "Такая задача носит название распознавания именованных сущностей (NER, Named Entity Recognition). Эту задачу мы будем решать при помощи нейросетей для анализа языка.\n",
        "\n",
        "Стоит отметить, что многие задачи обработки естественного языка (NLP, Natural Language Processing), решаются подобным образом, поэтому опыт и знания, полученные в этом ноутбуке можно использвать и для других задач."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmgSdh9JNYPd"
      },
      "source": [
        "\n",
        "Какие сущности мы будем распознавать:\n",
        "\n",
        "*   **PER, Персона**, например, Элвис Пресли, Одри Хепберн, Дэвид Бекхэм\n",
        "*   **ORG, Организация**, например, Google, Mastercard, Оксфордский университет\n",
        "\n",
        "Какие еще сущности входят в задачу NER\n",
        "*   **Дата-Время** например, 2006, 16:34, 2 часа ночи\n",
        "*   **Локация**, например, Трафальгарская площадь, МоМА, Мачу-Пикчу.\n",
        "*   **Произведение искусства**, например, «Гамлет», «Мона Лиза»."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_oj7bd6wvSm"
      },
      "source": [
        "## 0.2. План работы"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXPczR46wvSn"
      },
      "source": [
        "\n",
        "\n",
        "1. Подготовка данных\n",
        "    1. Загрузка данных и знакомство с форматом хранения\n",
        "    2. Анализ статистики данных\n",
        "    3. Упрощение данных\n",
        "    4. Нормализация данных\n",
        "    5. Создание скрипта датасета\n",
        "2. Подготовка нейронной сети\n",
        "3. Функция потерь\n",
        "4. Метрики\n",
        "5. Обучение модели\n",
        "6. Анализ ошибок\n",
        "7. Конверация в jit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80W2qgXBwvSo"
      },
      "source": [
        "# 1. Данные"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdIEoJF2wvSo"
      },
      "source": [
        "## 1.1. Общая информация"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8L5AV_Nwx7O"
      },
      "source": [
        "В этом ноутбуке мы будет работать с данными из соревнования [RuNNE](https://github.com/dialogue-evaluation/RuNNE) -  \"извлечение именованных сущностей в few-shot режиме\".\n",
        "Главная особенность этого набора данных - множественная вложенность сущностей (включая вложенность сущностей одного типа).\n",
        "\n",
        "В этой тетрадке мы не будем говорить о few-shot подходах, по этой теме есть наша [статья на хабр](https://habr.com/ru/company/sberbank/blog/649609/). В ней можно познакомиться с инсайдами по данной теме и узнать, что явлется SOTA. Кроме того, работать с вложенностью мы пока тоже не будем. \n",
        "\n",
        "После проведения анализа мы упростим задачу и оставим только неперсекающие сущности: слона будем есть по частям :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPAB_W1TwvSp"
      },
      "source": [
        "## 1.2. Загрузка данных"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqm0pU-qIiD4"
      },
      "source": [
        "Данные, которые мы будем использовать в этом ноутбуке, хранятся в формате **BRAT**.\n",
        "\n",
        "Наша задача -- считать данные из хранилища и преобразовать их таким образом, чтобы они были удобны для обучения нейронной сети.\n",
        "\n",
        "Вообще **BRAT** -- это онлайн-инструмент для разметки письменных текстов. Детальное описание можно найти [здесь](https://brat.nlplab.org/introduction.html)\n",
        "\n",
        "Также **BRAT** -- это и формат хранения данных, который был разработан для этого инструмента. Именно в этом формате лежат те данные, которые нас интересуют в этом ноутбуке.\n",
        "\n",
        "Функция ```read_annotation``` позволяет считывать данные в этом формате. (Код доступен в папке ```utils```).\n",
        "\n",
        "В этой функции происходит следующее:\n",
        "* Для каждого текста в датасете:\n",
        "    * Считывается текст из файла для текста\n",
        "    * Из соседнего файла для разметки считывается разметка в формате ```(str: Label, long: Start, long: End)```\n",
        "\n",
        "Результат складывается в ```list```, в котором хранится полный набор данных.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tg-nCxo0wvSq"
      },
      "source": [
        "### 1.2.1. Загрузка данных"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = repo_folder + \"IntelligentDocumentProcessing/Resources/4_Named_Entity_Recognition/\"\n",
        "!unzip -q {data_path + \"RuNNE.zip\"} -d {data_path}"
      ],
      "metadata": {
        "id": "-jyGtpOdFpz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "db3-0W26t1xN"
      },
      "outputs": [],
      "source": [
        "!head -n 10 {data_path + \"RuNNE/data/train/003.ann\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOIvep4ut1xO"
      },
      "outputs": [],
      "source": [
        "!head -n 6 {data_path + \"RuNNE/data/train/003.txt\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAviEbtCg4Wy"
      },
      "outputs": [],
      "source": [
        "from utils import read_annotation\n",
        "\n",
        "train_data = read_annotation(data_path + \"RuNNE/data/train\", \"RuNNE-Train\")\n",
        "valid_data = read_annotation(data_path + \"RuNNE/data/dev\", \"RuNNE-Dev\")\n",
        "test_data = read_annotation(data_path + \"RuNNE/data/test\", \"RuNNE-Test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vXtrUM_wvSr"
      },
      "source": [
        "### 1.2.2. Проверка работы"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7V0GgntCshMe"
      },
      "outputs": [],
      "source": [
        "datum = train_data[0]\n",
        "print(\"====== Первые 100 символов текста ======\")\n",
        "print(datum[0][:100])\n",
        "print(\"\")\n",
        "print(\"====== Первые 10 сущностей ======\")\n",
        "print(datum[1][:10])\n",
        "print(\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S16ll9kgsf4o"
      },
      "source": [
        "Формат вывода не самый информативный. [ipymarkup](https://github.com/natasha/ipymarkup) - удобая библиотека, которая позволяет отобразить разметку более наглядно, используя HTML.\n",
        "\n",
        "[Документация](http://nbviewer.jupyter.org/github/natasha/ipymarkup/blob/master/docs.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILxH1nqsf36Z"
      },
      "outputs": [],
      "source": [
        "show_span_line_markup(*train_data[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bg1OlwOj37Le"
      },
      "source": [
        "Такой вариант выглядит гораздо более понятно"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEjPJKLwwvSs"
      },
      "source": [
        "## 1.3. Разбиение на предложения и на слова. Токенизация"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FMqkqQhwvSt"
      },
      "source": [
        "Мы загрузили тексты из датасета. С тем, что уже есть можно начинать работу: разобьем текст на предложения, а предложения на слова -- и дальше работаем со словами.\n",
        "\n",
        "Но, как мы разбирали в лекции, задача значительно упростится, если текст предобработать: токенизировать.\n",
        "\n",
        "Для токенизации существуют отдельные модели. В нашем случае мы будем использовать BERT токенизатор. Да, тот самый BERT, из которого мы будем делать нашу модель чуть позже.\n",
        "\n",
        "Токенизацию можно делать при помощи множества различных инструментов. Это могут быть как эвристические решения, так и нейросетевые модели."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rh0Ubm1SORHM"
      },
      "source": [
        "### 1.3.1. Сколько каждый документ содержит предложений?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyM8apR0t1xQ"
      },
      "source": [
        "#### Задача 0. \n",
        "Сегментация с сохранением координат. Необходимо воспользоваться библиотекой RuSentTokenize, а также добавить разбиение по новым строкам.\n",
        "На вход: список новостей\n",
        "На выход: список предложений с координатами"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z45zZDcet1xR"
      },
      "outputs": [],
      "source": [
        "from rusenttokenize import ru_sent_tokenize\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkfGWSB4t1xR"
      },
      "outputs": [],
      "source": [
        "# Заготовка\n",
        "\n",
        "def sentence_segmentation(\n",
        "    texts: List[str], return_coords: bool = False\n",
        ") -> List[List[str]]:\n",
        "    sent_per_text = []\n",
        "    \"\"\"\n",
        "    Ваш код здесь\n",
        "    \"\"\"\n",
        "    return sent_per_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXou5jBkt1xR"
      },
      "outputs": [],
      "source": [
        "# Проверка\n",
        "\n",
        "check_text = \"Д.Медведев снял с должности замсекретаря Совбеза РФ Ю.Балуевского\\n\\nПрезидент России Дмитрий Медведев освободил Юрия Балуевского от должности замсекретаря Совета безопасности России. Соответствующий указ опубликован на сайте государственной системы правовой информации.\\n\\nУказ глава государства подписал 9 января.\"\n",
        "\n",
        "sents_with_coords = [\n",
        "    (0, 65, 'Д.Медведев снял с должности замсекретаря Совбеза РФ Ю.Балуевского'),\n",
        "    (67, 181, 'Президент России Дмитрий Медведев освободил Юрия Балуевского от должности замсекретаря Совета безопасности России.'),\n",
        "    (182, 268,'Соответствующий указ опубликован на сайте государственной системы правовой информации.'),\n",
        "    (270, 311, 'Указ глава государства подписал 9 января.')\n",
        "]\n",
        "sents_without_coords = [\n",
        "    'Д.Медведев снял с должности замсекретаря Совбеза РФ Ю.Балуевского',\n",
        "    'Президент России Дмитрий Медведев освободил Юрия Балуевского от должности замсекретаря Совета безопасности России.',\n",
        "    'Соответствующий указ опубликован на сайте государственной системы правовой информации.',\n",
        "    'Указ глава государства подписал 9 января.'\n",
        "]\n",
        "\n",
        "for expected_res, bool_value in [(sents_with_coords, True),(sents_without_coords,False)]:\n",
        "    res = sentence_segmentation([check_text], bool_value)\n",
        "    assert res == expected_res\n",
        "    \n",
        "print(\"Good job\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9IzrP6-kCSs"
      },
      "outputs": [],
      "source": [
        "train_sents_per_doc = sentence_segmentation([text for text, _ in train_data])\n",
        "valid_sents_per_doc = sentence_segmentation([text for text, _ in valid_data])\n",
        "test_sents_per_doc = sentence_segmentation([text for text, _ in test_data])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7PqC5tQkSgB"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15,10))\n",
        "plt.title(\"Distribution of sentence number per document\")\n",
        "h = sns.histplot([len(i) for i in train_sents_per_doc], kde=True, color=\"red\")\n",
        "h = sns.histplot([len(i) for i in valid_sents_per_doc], kde=True)\n",
        "h.set_ylabel(\"Num. of docs\")\n",
        "h.set_xlabel(\"Number of sentence per doc\")\n",
        "h.legend(labels=[\"train\", \"valid\"])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jndEV2zAwvSu"
      },
      "source": [
        "Как работает BPE токенизатор:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zaAD_XRjlNQ"
      },
      "outputs": [],
      "source": [
        "# эти функции помогут нам сегментировать документы на предложения, токены и bpe-токены\n",
        "from transformers import BertTokenizer  # BPE токенизация"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lr516LUfwAl2"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('sberbank-ai/ruBert-base', do_lower_case=False, max_len=10000)\n",
        "print(\"====== TOKINIZER ======\")\n",
        "print(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ol2kstf_mPe6"
      },
      "outputs": [],
      "source": [
        "in_data = \"Мама мыла раму окон от компании СберОкна\"\n",
        "print(\"====== INPUT DATA ======\")\n",
        "print(in_data)\n",
        "print()\n",
        "print(\"====== OUTPUT DATA ======\")\n",
        "print(tokenizer.tokenize(\"Мама мыла раму окон от компании СберОкна\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOXNtfQG_wq0"
      },
      "source": [
        "### 1.3.2. Количество BPE токенов на документ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hvFfmSlxQr8"
      },
      "outputs": [],
      "source": [
        "train_bpe_per_docs = [tokenizer.tokenize(text) for text, _ in train_data]\n",
        "valid_bpe_per_docs = [tokenizer.tokenize(text) for text, _ in valid_data]\n",
        "\n",
        "\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.title(\"Distribution of bpe number per document\")\n",
        "h = sns.histplot([len(i) for i in train_bpe_per_docs], kde=True, color=\"red\")\n",
        "h = sns.histplot([len(i) for i in valid_bpe_per_docs], kde=True)\n",
        "h.legend(labels=[\"train\", \"valid\"])\n",
        "h.set_ylabel(\"Num. of docs\")\n",
        "h.set_xlabel(\"Number of bpe per doc\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1eun7iyQ7dN"
      },
      "source": [
        "#### Количество BPE токенов в предложениях"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFpI9H7XRsdD"
      },
      "outputs": [],
      "source": [
        "train_bpe_per_sentence = [tokenizer.tokenize(sentence) for sentence in train_sents_per_doc]\n",
        "valid_bpe_per_sentence = [tokenizer.tokenize(sentence) for sentence in valid_sents_per_doc]\n",
        "test_bpe_per_sentence = [tokenizer.tokenize(sentence) for sentence in test_sents_per_doc]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krIVgSO6x6Gp"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15,10))\n",
        "plt.title(\"Distribution of bpe number per sentence\")\n",
        "h = sns.histplot([len(i) for i in train_bpe_per_sentence], kde=True, color=\"red\")\n",
        "h = sns.histplot([len(i) for i in valid_bpe_per_sentence], kde=True)\n",
        "h.legend(labels=[\"train\", \"valid\"])\n",
        "h.set_ylabel(\"Num. of sentences\")\n",
        "h.set_xlabel(\"Number of bpe per sentence\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkvKP3WqbVPO"
      },
      "source": [
        "Теперь перейдем к тому, как вглядит разметка, так как именно соответствие текста и разметки -- залог обучения качественной модели."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XVv4VsJv-L8"
      },
      "source": [
        "#### Задача 1\n",
        "\n",
        "Как много уникальных текстов сущностей есть в каждом наборе данных (относительно всех текстов сущностей в данном наборе): \n",
        "\n",
        "`set(train_entities)/list(train_entities)`. \n",
        "\n",
        "Функция принимает на вход список семплов из набора, возвращает чет уникальных текстов сущностей (без привязки к типу) и список всех текстов сущностей. Округлить до 3 символа.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCJU693Ei2KG"
      },
      "outputs": [],
      "source": [
        "# Заготовка\n",
        "\n",
        "def unique_entity_texts(dataset: List[Tuple[str, List[Tuple[int, int, str]]]]):\n",
        "    ent_texts = [] \n",
        "    \"\"\"\n",
        "    Ваш код здесь\n",
        "    \"\"\"\n",
        "    unique_ent_texts = set(ent_texts)\n",
        "    print(f\"Entities in dataset: {len(ent_texts)}\")\n",
        "    print(f\"Unique entity texts in dataset: {len(unique_ent_texts)}\")\n",
        "    print(f\"Relation unique ent texts to all ent texts: {round(len(unique_ent_texts) / len(ent_texts), 3)}\")\n",
        "    return 0, 0 # изменить"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4JLWVKPen58A"
      },
      "outputs": [],
      "source": [
        "print(\"Train data\")\n",
        "unique_ent_texts_train, _ = unique_entity_texts(train_data)\n",
        "print(\"\\nValid data\")\n",
        "unique_ent_texts_valid, _ = unique_entity_texts(valid_data)\n",
        "print(\"\\nTest data\")\n",
        "unique_ent_texts_test, _ = unique_entity_texts(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_o8v2zno4dy"
      },
      "source": [
        "##### Проверка"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OlXVSV69o2iI"
      },
      "outputs": [],
      "source": [
        "unique_ent_texts, ent_texts = unique_entity_texts(train_data)\n",
        "assert len(unique_ent_texts) == 15831, \"Неверное количество уникальных текстов сущностей в тренировочном наборе\"\n",
        "assert len(ent_texts) == 27743, \"Неверное количество сущностей в тренировочном наборе\"\n",
        "\n",
        "print(\"Good job\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4nNfn1XwvSz"
      },
      "source": [
        "#### Задача 2\n",
        "\n",
        "Какое количество уникальных текстов сущностей, присутствующих в валидационном и тестовом наборах, но не содержащихся в тренировочном? \n",
        "\n",
        "Функция принимает на вход три сета с уникальными наборами текстов сущностей, возвращает два значения intersection over union: между валидационной выборкой и тренировочной, между тестовой выборкой и тренировочной. Округлить до 4 символа"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XaAaAhXQt1xV"
      },
      "outputs": [],
      "source": [
        "# Заготовка\n",
        "def iou(\n",
        "    first_set: Set[str], \n",
        "    second_set: Set[str]\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Ваш код здесь\n",
        "    \"\"\"\n",
        "    return 0\n",
        "\n",
        "def general_iou(\n",
        "    unique_ent_texts_train: Set[str], \n",
        "    unique_ent_texts_valid: Set[str], \n",
        "    unique_ent_texts_test: Set[str]\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Ваш код здесь\n",
        "    \"\"\"\n",
        "    return 0, 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B--xMcQLSi8O"
      },
      "source": [
        "##### Проверка"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JOrJWbTwvSz"
      },
      "outputs": [],
      "source": [
        "unique_ent_texts_train, _ = unique_entity_texts(train_data)\n",
        "unique_ent_texts_valid, _ = unique_entity_texts(valid_data)\n",
        "unique_ent_texts_test, _ = unique_entity_texts(test_data)\n",
        "\n",
        "valid_train_iou, test_train_iou = general_iou(unique_ent_texts_train, unique_ent_texts_valid, unique_ent_texts_test)\n",
        "\n",
        "assert valid_train_iou == 0.0535, \"Неверное значение IOU для набора валидация vs трейн\"\n",
        "assert test_train_iou == 0.0542, \"Неверное значение IOU для набора тест vs трейн\"\n",
        "print(\"Good job\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJuPQ-30wvS0"
      },
      "source": [
        "## 1.4. Анализ разметки"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqeII6l3RDr_"
      },
      "source": [
        "### 1.4.1. Как пересекаются сущности?\n",
        "\n",
        "- Есть сущности разных типов, которые не персекаются\n",
        "- Есть сущности разных типов, которые персекаются\n",
        "- Есть сущности одного и того же типа, которые пересекаются "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Czvk65im2TTK"
      },
      "outputs": [],
      "source": [
        "show_span_line_markup(*train_data[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WD58SJTRvoWc"
      },
      "source": [
        "## 1.5. Схема теггирования"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UP56C4ZwvS1"
      },
      "source": [
        "В задаче NER бывает несколько типов разметки\n",
        "* **IO**: самая простая разметка. В ней токены делятся на два типа: \n",
        "    * **I**, Inside, токены, принадлежащие именованной сущности\n",
        "    * **O**, Outside, токены, не входящие в именованную сущность\n",
        "Ограничение: нельзя корректно соединить несколько последовательных слов, из которых состоит одна именованная сущность.\n",
        "\n",
        "* **BIO**: более продвинутая разметка. Токены делятся на три типа:\n",
        "    * **B**, Beginning, токены, с которых начинается именованная сущность\n",
        "    * **I**, Inside, токены, которые входят в именованную сущность, но с них именованная сущность не начинается\n",
        "    * **O**, Outside, токены, которые не входят в именованную сущность\n",
        "При помощи этого типа разметки можно решить проблему с именованными сущностями, состоящими из нескольких слов.\n",
        "\n",
        "[More info](https://www.sciencedirect.com/science/article/pii/S1110866520301596)\n",
        "\n",
        "\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://devopedia.org/images/article/256/4681.1580659482.png' />\n",
        "<figcaption>The examples of tagging according to different schemes </figcaption></center>\n",
        "</figure>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcxsXFrlaUar"
      },
      "source": [
        "### Задача 3\n",
        "Определите класс, который умеет конвертировать исходную разметку в BIO схему теггирования и обратно.\n",
        "\n",
        "Нужно реализовать три метода:\n",
        "\n",
        "**markup2scheme** - принимает на вход кортеж из токенов и исходной разметки со спанами, возвращает токены и список с лейблами на каждый токен\n",
        "\n",
        "**scheme2markup** - принимает на вход кортеж из токенов и лейблов на каждый токен, возвращает токены и разметку ввиде спанов\n",
        "\n",
        "**run** - выполняет один из методов выше, в соответствии с флагом"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zh2oaG3iaUIw"
      },
      "outputs": [],
      "source": [
        "# Заготовка\n",
        "\n",
        "class BIOMarkuper:\n",
        "\n",
        "    def __init__(self, out_value: str = \"O\"):\n",
        "        self.out_value = out_value\n",
        "\n",
        "    def markup2scheme(\n",
        "        self, \n",
        "        input_sample: Tuple[\n",
        "                        List[Tuple[int, int, str]], # tokens\n",
        "                        List[Tuple[int, int, str]]  # entities\n",
        "                      ]\n",
        "    ) -> Tuple[List[str], List[str]]:\n",
        "      \"\"\"\n",
        "      Умеет преобразовывать исходный пример в виде токенов (с координатами) и списка сущностей \n",
        "      в экземпляр обучения, то есть список токенов и список лейблов, составеленных в соответствии с BIO схемой.\n",
        "      \"\"\"\n",
        "      pass\n",
        "      \n",
        "    def char2token_markup(\n",
        "        self, \n",
        "        tokens: List[Tuple[int, int, str]], \n",
        "        ents: List[Tuple[int, int, str]]\n",
        "    ) -> List[Tuple[int, int, str]]:\n",
        "      \"\"\"\n",
        "      Маппит символьные координаты для каждой сущности в токенных.\n",
        "      \"\"\"\n",
        "      pass\n",
        "    \n",
        "    def scheme2markup(\n",
        "        self,\n",
        "        output_sample: Tuple[List[str], List[str]]\n",
        "    ) -> Tuple[List[str], List[Tuple[int, int, str]]]:\n",
        "      \"\"\"\n",
        "      Умеет преобразовывать экземпляр обчения в виде токенов (с координатами) и списка лейблов, составеленных в соответствии с BIO схемой \n",
        "      в исходный пример, то есть список токенов и спискок сущностей с символьными координатами.\n",
        "      \"\"\"\n",
        "      pass\n",
        "\n",
        "    def token2char_markup(\n",
        "        self, \n",
        "        tokens: List[Tuple[int, int, str]], \n",
        "        ents: List[Tuple[int, int, str]]\n",
        "    ) -> List[Tuple[int, int, str]]:\n",
        "      \"\"\"\n",
        "      Маппит токенные координаты для каждой сущности в символьные.\n",
        "      \"\"\"\n",
        "      pass\n",
        "\n",
        "    def run(self, \n",
        "            sample: Union[\n",
        "                            Tuple[ # input_semple\n",
        "                                List[Tuple[int, int, str]], # tokens\n",
        "                                List[Tuple[int, int, str]]  # entities\n",
        "                            ],\n",
        "                            Tuple[\n",
        "                                List[Tuple[int, int, str]], # tokens\n",
        "                                List[str]  # scheme\n",
        "                            ]\n",
        "                          ],\n",
        "            direction: str):\n",
        "      \"\"\"\n",
        "      Запускает конвертацию одного примера в нужном направлении.\n",
        "      \"\"\"\n",
        "      pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9jeNwzKc8vf"
      },
      "source": [
        "#### Проверка"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhUsNQ99c8HD"
      },
      "outputs": [],
      "source": [
        "tokens = [(0, 6, \"Иванов\"), (7, 11, \"Иван\"), (12, 18, \"поедет\"), (19, 20, \"в\"), (21, 26,\"Санкт\"), (26, 27,\"-\"), (27, 36, \"Петербург\")]\n",
        "entities = [(0, 11, \"PER\"), (21, 36, \"LOC\")]\n",
        "scheme = [\"B_PER\", \"I_PER\", \"O\", \"O\", \"B_LOC\", \"I_LOC\", \"I_LOC\"]\n",
        "\n",
        "input_sample = (tokens, entities)\n",
        "output_sample = (tokens, scheme)\n",
        "markuper = BIOMarkuper()\n",
        "\n",
        "\n",
        "result = markuper.run(input_sample, \"m2s\")\n",
        "assert output_sample == result,  \"Incorrect convertion to the scheme\"\n",
        "result = markuper.run(output_sample, \"s2m\")\n",
        "assert input_sample == result,  \"Incorrect convertion to the markup\"\n",
        "print(\"Good job\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgHk1eYdHXcE"
      },
      "source": [
        "## 1.6. Упрощаем задачу"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Xgo9hiJwvS2"
      },
      "source": [
        "Пересечения -- это проблема. Потому что нужно, чтобы модель имела возможность предсказывать сразу несколько именованных сущностей, причем, возможно, одного класса.\n",
        "\n",
        "Поэтому, как и говорилось выше, решать сразу соревнование RuNNE мы не будем. Вместо этого мы возьмем только две базовых сущности, у которых нет вложенности и нет пересечения. \n",
        "\n",
        "Итоговая задача будет выглядеть следующим образом: \n",
        "1. Будем работать только в пределах предложений - производим сегментацию.\n",
        "2. Разметку нужно обработать - раньше координаты начала и конца были на уровне документов, а нам нужны на уровне предложений.\n",
        "3. Удаляем сущности все сущности с типом отличным от **PERSON**, **ORGANIZATION**. Распознавать будем только эти сущности.\n",
        "\n",
        "Эти операции проделает код, который приведен ниже. Он очень прост с точки зрения реализации, поэтому можете просто ознакомиться с тем, что в нем происходит или пропустить, если вы понимаете, как реализовать преобразования выше."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASZLczrdt1xY"
      },
      "outputs": [],
      "source": [
        "from utils import shift_entities\n",
        "\n",
        "def add_markup_to_each_sentence(data: list):\n",
        "    flatten_data_with_shift_ents = []\n",
        "    for document_text, markup in data:\n",
        "        sentences_infos = sentence_segmentation([document_text], True)\n",
        "        ent_per_sents = shift_entities(sentences_infos, markup)\n",
        "        for sent_info, sent_ent, in zip(sentences_infos, ent_per_sents):\n",
        "            flatten_data_with_shift_ents.append((sent_info, sent_ent))\n",
        "    return flatten_data_with_shift_ents\n",
        "\n",
        "train_sentence_samples = add_markup_to_each_sentence(train_data)\n",
        "print()\n",
        "valid_sentence_samples = add_markup_to_each_sentence(valid_data)\n",
        "print()\n",
        "test_sentence_samples = add_markup_to_each_sentence(test_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7t6y5LesHyI8"
      },
      "outputs": [],
      "source": [
        "for (_, _, text), markup in train_sentence_samples[:4]:\n",
        "  show_span_line_markup(text, markup)\n",
        "  print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTb9xrTJIySj"
      },
      "outputs": [],
      "source": [
        "from utils import filter_entities\n",
        "\n",
        "needed_ents = {'PERSON', 'ORGANIZATION'}\n",
        "valid_sentence_samples = filter_entities(valid_sentence_samples, needed_ents)\n",
        "train_sentence_samples = filter_entities(train_sentence_samples, needed_ents)\n",
        "test_sentence_samples = filter_entities(test_sentence_samples, needed_ents)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Посмотрим сколько у нас примеров в каждой из выборок:"
      ],
      "metadata": {
        "id": "sddnWHj4NW3e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q28dgW-UF424"
      },
      "outputs": [],
      "source": [
        "print(f\"Train samples: {len(train_sentence_samples)}\")\n",
        "print(f\"Valid samples: {len(valid_sentence_samples)}\")\n",
        "print(f\"Test samples: {len(test_sentence_samples)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1x4X5Niii2e"
      },
      "source": [
        "Модель будем обучать и валидировать только на примерах, в которых есть сущности, это также наше упрощение задачи."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHKQpRNAWzeK"
      },
      "outputs": [],
      "source": [
        "train = [i for i in train_sentence_samples if i[1]]\n",
        "valid = [i for i in valid_sentence_samples if i[1]] \n",
        "test = [i for i in test_sentence_samples if i[1]] \n",
        "print(f\"Train samples: {len(train)}\")\n",
        "print(f\"Valid samples: {len(valid)}\")\n",
        "print(f\"Test samples: {len(test)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFQlNfUfwvS4"
      },
      "source": [
        "Мы подготовили данные так, чтобы они были удобны для обучения нейронной сети."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8x10VtiCHe-o"
      },
      "source": [
        "## 1.7 Подготовка DataLoader\n",
        "\n",
        "Для этого нам понадобятся стандартные `torch` инстурменты: `Dataset` и `DataLoader`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfInMuZ2wvS4"
      },
      "source": [
        "### 1.7.1. Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAQtYWDG_qPF"
      },
      "source": [
        "\n",
        "  1. Определяем паддинги (токен и индекс) и максимальную длину последовательности в соответствии с моделью\n",
        "  2. Инициализируем два токенизатора: токенный - `WordPunctTokenizer`, bpe - `BertTokenizer`, то есть будем сначала разбивать предложения на токены, а потом токены на bpe.\n",
        "  3. При инициализации класса готовим для каждого примера  тензорное представление\n",
        "\n",
        "\n",
        "Как мы приводим к тензорному представлению:\n",
        "\n",
        "- Сегментируем на токены исходные предложения. \n",
        "- Маппим символьные спаны сущностей на метки токенов. \n",
        "\t\n",
        "```\n",
        "tokens = [\"Мама\" , \"мыла\", \"раму\", \"окон\", \"фирмы\", \"СберОкна\"]\n",
        "token_labels = [\"O\" , \"O\", \"O\", \"O\", \"O\", \"ORGANIZATION\"]\n",
        "```\n",
        "- Токенизируем токены на bpe части и конвертируем их в индекс в соответствии с индексом хранящимся в BertTokenizer. При этом если из одного токена мы получается несколько bpe мы расширяем список с labels:\n",
        "\n",
        "```\n",
        "bpe_tokens = [\"Сбер\", \"##О\", \"##к\", \"##н\"]\n",
        "input_ids = [86, 13, 10003, 65999]\n",
        "labels = [2, 2, 2, 2, 2]\n",
        "token_start_mask_id = [2, -100, -100, -100, -100]\n",
        "```\n",
        "- `labels` будем использовать для обучения и считать по ним лосс, а - `token_start_mask_id` - для конвертации предсказаний моедли, метка токена (тип сущности) будет сниматься только с первого bpe токена.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5O7bddst1xa"
      },
      "source": [
        "Один из основных классов в нашей тетрадке `CustomDataset`. У нас есть примеры, которые мы уже довольно хорошо изучили.\n",
        "Теперь нам нужно понять как их подавать в модель. И тут небольшая подсказка `BERT-like` модели ожадают на вход как минимум\n",
        "`\"input_ids\"`, `\"attention_mask\"`, `\"labels\"` для обучения. В процессе реализации CustomDataset, вы поймете, что есть что."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "wCot_AqBt1xa"
      },
      "source": [
        "#### Задача 4.\n",
        "Создание \"ленивого\" (вполовину) загрузчика данных - имплементация класса `CustomDataset`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "MKTz05fjt1xa"
      },
      "outputs": [],
      "source": [
        "# Заготовка\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(\n",
        "            self, \n",
        "            samples: list, \n",
        "            out_label: str = \"O\", \n",
        "            max_seq_len: int = 512,\n",
        "            pad_index: int = -100\n",
        "\n",
        "    ):\n",
        "\n",
        "        self.out_label = out_label\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.pad_index = pad_index\n",
        "\n",
        "\n",
        "        # в первом эксперименте мы будем использовать IO схему теггирования, \n",
        "        # если немного поразмышлять, то поймем - IO схема, значит что никаких префиксов к типам сущностей мы можем и не добавлять\n",
        "        labels = [out_label, 'PERSON', 'ORGANIZATION']\n",
        "        self.label_to_id = {label: index for index, label in enumerate(labels)}\n",
        "\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('sberbank-ai/ruBert-base', max_len=10000)\n",
        "        # так как интерфейс tokenizers в HF одинаковый, то вы сможете легко попробовать и другие модели\n",
        "        # self.tokenizer = RobertaTokenizerFast.from_pretrained('blinoff/roberta-base-russian-v0', max_len=10000)\n",
        "\n",
        "        self.tokens_per_sample, self.data = [], []\n",
        "\n",
        "        self.wpt = WordPunctTokenizer()\n",
        "\n",
        "        for sentence_info, markup in tqdm(samples):\n",
        "            sample_tensors, tokens_positions = self.prepare_sentence(\n",
        "                sentence_info, markup, self.wpt.span_tokenize\n",
        "            )\n",
        "            self.data.append(sample_tensors)\n",
        "            self.tokens_per_sample.append(tokens_positions)\n",
        "\n",
        "    def prepare_sentence(\n",
        "          self, sentence_info: tuple, ents: list, tokenize_fn: Callable\n",
        "    ) -> Tuple[Dict[str, torch.tensor], List[Tuple[int, int, str]]]:\n",
        "        \"\"\"\n",
        "        Задача:\n",
        "        1. Разбить текст входного предложения на токены с координатами, то есть\n",
        "            реализовать метод tokenize_sentence\n",
        "\n",
        "        2. Подготовить список лейблов (на каждый токен):\n",
        "            - сначала проинициализируйте все падами\n",
        "            - потом заполните пады реальными лейблами: для этого используйте\n",
        "              координаты токенов и координаты сущностей в символах\n",
        "            то есть реализуйте метод convert_span2labels\n",
        "        3. Инициализируем списки labels_ids, token_start_mask со значением -100\n",
        "        4. Инициализируем tokens_ids с одним элементом - индекс токена CLS (используйте \n",
        "            метод convert_tokens_to_ids у BPE токенизатора)\n",
        "\n",
        "        5. Идем циклом сразу по двум спискам токены и лейблы\n",
        "            - токен разбиваем на bpe\n",
        "            - переводим bpe в индексы\n",
        "            - добавляем в список tokens_ids\n",
        "            - конвертируем лейбл в индекс и дублируем столько раз,\n",
        "              сколько получилось бпе в токене\n",
        "            - добавляем в список labels_ids\n",
        "            - создаем на каждый токен token_start_mask_id,\n",
        "              где первый элемент индекс лейбл, а остальные -100 (будем потом \n",
        "                  использовать для конвертации и расчета метрик)\n",
        "            - добавляем список token_start_mask_id к token_start_mask \n",
        "\n",
        "        6. Паддим до максимальной длины или обрезаем списки\n",
        "        7. Конвертируем в тензора и собираем тензора в словарь\n",
        "\n",
        "        \"\"\"\n",
        "        s_start, s_end, sentence = sentence_info\n",
        "\n",
        "        tokens_per_sample = self.tokenize_sentence(sentence, tokenize_fn)\n",
        "\n",
        "        labels = self.convert_span2labels(self.out_label, ents, tokens_per_sample)\n",
        "\n",
        "        \"\"\"\n",
        "        Ваш код здесь\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenize_sentence(sentence: str, tokenize_fn: Callable):\n",
        "        \"\"\"\n",
        "        На вход ожидается предложение и функция для токенизация,\n",
        "        а на выходе список из туплов с информацией о расположении токенов\n",
        "        \n",
        "        Я иду домой -> (0, 1, я) (2, 5, иду) (6, 11, домой)\n",
        "        \n",
        "        \"\"\"\n",
        "        tokens_per_sample = []\n",
        "        \"\"\"\n",
        "        Ваш код здесь\n",
        "        \"\"\"\n",
        "        return tokens_per_sample\n",
        "\n",
        "    @staticmethod\n",
        "    def convert_span2labels(\n",
        "            out_label: str, ents: list, tokens_per_sample: list\n",
        "    ):\n",
        "        \"\"\"\n",
        "        На вход ожидается значение out_label токена, список сущностей и\n",
        "        список токенов с мета информацией. Пересекаем здесь сущности\n",
        "        по координатам с токенами и возвращемс список лейблов.\n",
        "        Количество лейблов должно быть равно количеству токенов.\n",
        "        \"\"\"\n",
        "        labels = [out_label] * len(tokens_per_sample)\n",
        "        \"\"\"\n",
        "        Ваш код здесь\n",
        "        \"\"\"\n",
        "        return labels\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.data[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Проверка\n",
        "test_sample = [\n",
        "    (\n",
        "        (0, 36,\"Иванов Иван поедет в Сбербанк\"), [(0, 11, \"PERSON\"), (21, 29, \"ORGANIZATION\")]\n",
        "     )\n",
        "]\n",
        "\n",
        "assert_cd = CustomDataset(test_sample, max_seq_len=15)\n",
        "\n",
        "first_prepared_sentence = assert_cd.data[0]\n",
        "\n",
        "print(\"=\" * 40)\n",
        "\n",
        "print(\"input_ids: \\n\", first_prepared_sentence[\"input_ids\"], \"\\n\")\n",
        "print(\"attention_mask: \\n\", first_prepared_sentence[\"attention_mask\"], \"\\n\")\n",
        "print(\"labels: \\n\", first_prepared_sentence[\"labels\"], \"\\n\")\n",
        "print(\"token_start_mask: \\n\", first_prepared_sentence[\"token_start_mask\"], \"\\n\")\n",
        "\n",
        "\n",
        "assert torch.equal(first_prepared_sentence[\"input_ids\"], torch.tensor([100, 104691, 823, 104691, 379, 31888, 113,  83492, 9635, 0, 0, 0, 0, 0, 0]))\n",
        "assert torch.equal(first_prepared_sentence[\"attention_mask\"], torch.tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.]))\n",
        "assert torch.equal(first_prepared_sentence[\"labels\"], torch.tensor([-100, 1, 1, 1, 1, 0, 0, 2, 2, -100, -100, -100, -100, -100, -100]))\n",
        "assert torch.equal(first_prepared_sentence[\"token_start_mask\"], torch.tensor([-100, 1, -100, 1, -100, 0, 0, 2, -100, -100, -100, -100,-100, -100, -100]))\n",
        "\n",
        "print(\"=\" * 40)\n",
        "\n",
        "print(\"good job\")"
      ],
      "metadata": {
        "id": "GrbtmzKUZP77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJ4ESwmet1xb"
      },
      "source": [
        "Инциализируем датасеты для всех набором и посмотрим как в тензорном виде выглядит один пример для обучения:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVZrg11axRz4"
      },
      "outputs": [],
      "source": [
        "MAX_SEQ_LEN = 256\n",
        "train_ds = CustomDataset(train, max_seq_len=MAX_SEQ_LEN)\n",
        "valid_ds = CustomDataset(valid, max_seq_len=MAX_SEQ_LEN)\n",
        "test_ds = CustomDataset(test, max_seq_len=MAX_SEQ_LEN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQmYnmgI3jaG"
      },
      "source": [
        "### 1.7.2. Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZ8_GYTtzaPq"
      },
      "outputs": [],
      "source": [
        "NUM_WORKERS = 2\n",
        "train_dl = DataLoader(train_ds, batch_size=16, num_workers=NUM_WORKERS)\n",
        "valid_dl = DataLoader(valid_ds, batch_size=16, num_workers=NUM_WORKERS)\n",
        "test_dl = DataLoader(test_ds, batch_size=16, num_workers=NUM_WORKERS, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdfeGO8G52Y-"
      },
      "source": [
        "# 2. Модель"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtFyUZCEHk5k"
      },
      "source": [
        "## 2.1. Подготовка модели и Lightning модуля"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVeY4Dozt1xf"
      },
      "source": [
        " ### Основная модель: [BertForTokenClassification](https://github.com/huggingface/transformers/blob/v4.21.3/src/transformers/models/bert/modeling_bert.py#L1709)\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://d2l.ai/_images/bert-tagging.svg'/>\n",
        "</center>\n",
        "</figure>\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "### Что именно инициализируется и как?\n",
        "\n",
        "```python\n",
        "class BertForTokenClassification(BertPreTrainedModel):\n",
        "\n",
        "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "\n",
        "        self.bert = BertModel(config, add_pooling_layer=False)\n",
        "        classifier_dropout = (\n",
        "            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n",
        "        )\n",
        "        self.dropout = nn.Dropout(classifier_dropout)\n",
        "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
        "\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "```\n",
        "### А что еще реализовано за нас?\n",
        "\n",
        "```python\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: Optional[torch.Tensor] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        token_type_ids: Optional[torch.Tensor] = None,\n",
        "        position_ids: Optional[torch.Tensor] = None,\n",
        "        head_mask: Optional[torch.Tensor] = None,\n",
        "        inputs_embeds: Optional[torch.Tensor] = None,\n",
        "        labels: Optional[torch.Tensor] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None,\n",
        "    ) -> Union[Tuple[torch.Tensor], TokenClassifierOutput]:\n",
        "        r\"\"\"\n",
        "        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
        "            Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "\n",
        "        sequence_output = self.dropout(sequence_output)\n",
        "        logits = self.classifier(sequence_output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (logits,) + outputs[2:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return TokenClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### Self-attention"
      ],
      "metadata": {
        "id": "7iKME0enl0T0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "А теперь давайте разберемся, как работает механизм внимание. Для этого нам нужно создать несколько тензоров размерностью:\n",
        "- размер батча \n",
        "- длина последовательности \n",
        "- размером эмбеддингов \n",
        "\n",
        "После этого имплементируем механизм внимание, представленный в оригинальной статье."
      ],
      "metadata": {
        "id": "of3gm3UVd0RW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Шаги:\n",
        "\n",
        "\n",
        "- Инициализируем линейные преобразования\n",
        "- Инициализируем входную последовательность\n",
        "- Преобразуем входную последовательность"
      ],
      "metadata": {
        "id": "hmEIJw5Oj8eV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "input_sequence = torch.rand(10, 15, 20)\n",
        "\n",
        "k = nn.Linear(20, 17)\n",
        "q = nn.Linear(20, 17)\n",
        "v = nn.Linear(20, 17)\n",
        "\n",
        "key = k(input_sequence)\n",
        "query = q(input_sequence)\n",
        "value = v(input_sequence)"
      ],
      "metadata": {
        "id": "PVfQXvYsdzmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Получем attention_scores\n",
        "- Производим скейлинг последовательности"
      ],
      "metadata": {
        "id": "6Gn8nEw5kiF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attention_scores = torch.bmm(query, key.transpose(1, 2)) / key.size(-1)\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "\n",
        "sns.heatmap(attention_scores[0].detach().numpy())\n",
        "\n",
        "plt.title(\"Attention scores distribution\")\n",
        "plt.yticks(range(15), (f\"k-word {i}\" for i in range(15)), rotation=0)\n",
        "plt.xticks(range(15), (f\"q-word {i}\" for i in range(15)), rotation=20)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Eb0M7f1ch9-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attn_probs = F.softmax(attention_scores, -1)\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "\n",
        "sns.heatmap(attn_probs[0].detach().numpy())\n",
        "\n",
        "plt.title(\"Attention probs distribution\")\n",
        "plt.yticks(range(15), (f\"k-word {i}\" for i in range(15)), rotation=0)\n",
        "plt.xticks(range(15), (f\"q-word {i}\" for i in range(15)), rotation=20)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wDPO5VEXh_I4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- получаем сontetualized embeddings"
      ],
      "metadata": {
        "id": "LWgTKvehlKGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "contextulized_embeddings = torch.bmm(attn_probs, value)"
      ],
      "metadata": {
        "id": "oyOL74xvksAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure>\n",
        "<center>\n",
        "<img src='https://i.pinimg.com/originals/f9/ec/d9/f9ecd9df0dda3b85b93e19364ec1618e.jpg' />\n",
        "</center>\n",
        "</figure>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YUHgPkkDlXNh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "veg9P7gZG3g0"
      },
      "outputs": [],
      "source": [
        "import pytorch_lightning as pl\n",
        "\n",
        "\n",
        "class NERModel(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, num_labels: int = 3):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # self.model = RobertaForTokenClassification.from_pretrained('blinoff/roberta-base-russian-v0', num_labels=3)\n",
        "        # Готовая модель из transofomers\n",
        "        self.model = BertForTokenClassification.from_pretrained('sberbank-ai/ruBert-base', num_labels=num_labels)\n",
        "\n",
        "        # Сюда будем складвать данные по бачам из валидации, чтобы в конце эпохи рассчитать метрики\n",
        "        self.data_to_eval = []\n",
        "\n",
        "        self.index2label = {0: \"O\", 1: 'PERSON', 2:'ORGANIZATION'}\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "\n",
        "        out = self.model(\n",
        "            input_ids=batch[\"input_ids\"],\n",
        "            attention_mask=batch[\"attention_mask\"],\n",
        "            labels=batch[\"labels\"]\n",
        "            )\n",
        "        \n",
        "        self.log('train_loss', out.loss)\n",
        "\n",
        "        return out.loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "\n",
        "        out = self.model(\n",
        "            input_ids=batch[\"input_ids\"],\n",
        "            attention_mask=batch[\"attention_mask\"],\n",
        "            labels=batch[\"labels\"]\n",
        "            )\n",
        "        \n",
        "        self.log('val_loss', out.loss)\n",
        "        logits = out.logits.argmax(dim=-1)\n",
        "\n",
        "        self.data_to_eval.append(\n",
        "            (logits.cpu().numpy(), batch[\"token_start_mask\"].cpu().numpy())\n",
        "        )\n",
        "\n",
        "    def on_validation_end(self) -> None:        \n",
        "        self.count_metrics()\n",
        "        self.data_to_eval = []\n",
        "\n",
        "    def count_metrics(self):\n",
        "      \n",
        "      flat_preds, flat_target = [], []\n",
        "      for preditions, targets in self.data_to_eval:\n",
        "        for sample_pred, sample_target in zip(preditions, targets):\n",
        "          for index, s_t in enumerate(sample_target):\n",
        "            if s_t != -100:\n",
        "              flat_target.append(s_t)\n",
        "              flat_preds.append(sample_pred[index])\n",
        "      \n",
        "      report = classification_report(\n",
        "          flat_target, flat_preds, target_names = ['O','PERSON','ORGANIZATION'], output_dict = True\n",
        "      )\n",
        "      for key_report in report:\n",
        "        if key_report == \"accuracy\":\n",
        "          continue\n",
        "        self.trainer.logger.experiment.log(\n",
        "          {f\"F1-score: {key_report.upper()}\": report[key_report][\"f1-score\"]}\n",
        "        )\n",
        "        print(f\"F1-score - {key_report.upper()} : {report[key_report]['f1-score']}\")\n",
        "      print()\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=5e-6)\n",
        "        return optimizer\n",
        "\n",
        "    def predict(self, predict_dl: DataLoader):\n",
        "      self.eval()\n",
        "      preds = []\n",
        "      with torch.no_grad():\n",
        "        for batch in predict_dl:\n",
        "          batch = {k: v.to(self.device) for k, v in batch.items()}\n",
        "          out = self.model(\n",
        "            input_ids=batch[\"input_ids\"],\n",
        "            attention_mask=batch[\"attention_mask\"]\n",
        "            )\n",
        "          predictions = out.logits.argmax(dim=-1).cpu().numpy()\n",
        "          preds.extend(\n",
        "              self.format_predictions(predictions, batch[\"token_start_mask\"])\n",
        "              )\n",
        "      return preds\n",
        "          \n",
        "    def format_predictions(self, predictions: torch.tensor, tokens_mask: torch.tensor):\n",
        "      \"\"\"\n",
        "      Метод предназначен для создания лейблов из предсказанных логитов модели.\n",
        "      У нас есть тензор-маска - tokens_mask. Используем значение -100, чтобы только \n",
        "      с этих токенов снимать лейблы. Остальные логиты нас не интересуют. Поясню:\n",
        "      \n",
        "      С чем работает модель:\n",
        "      BPE         [Абра #кад #абра искомое слово]\n",
        "      predictions [2    2   2    0      0   ]\n",
        "      tokens_mask [-100 0   0    -100   -100]\n",
        "\n",
        "      С чем хотим работать мы:\n",
        "      Токены       [Абракадабра    искомое слово]\n",
        "      labels      [2             0      0   ]\n",
        "\n",
        "      \"\"\"\n",
        "      batch_preds = []\n",
        "      for pred, mask in zip(predictions, tokens_mask):\n",
        "\n",
        "        batch_preds.append(\n",
        "            [self.index2label[pred[i_m]] for i_m, mask_value in enumerate(mask) if mask_value != -100]\n",
        "        )\n",
        "      return batch_preds\n",
        "          \n",
        "              "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HaAH9ElsPfqk"
      },
      "outputs": [],
      "source": [
        "m = NERModel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMBpAK8I3tRm"
      },
      "source": [
        "# 3. Процесс обучения модели"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6vq0vNet1xg"
      },
      "source": [
        "Теперь мы готовы к обучению.\n",
        "\n",
        "Из того, что нам обязательно стоит сделать: \n",
        "\n",
        "1.   Инициализировать инстурмент трекинга экспериментов - мы продолжим работать с `WandbLogger`\n",
        "2.   Инициализировать инстурмент сохранения чекпоинтов, который позволит там сохранять лучшие веса моделей - `ModelCheckpoint`\n",
        "\n",
        "\n",
        "Поставим всего 5 эпох обучения - задача у нас довольно простая, данных достаточно для дообучения модели.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PqKFU0RXR_-"
      },
      "outputs": [],
      "source": [
        "!mkdir check_points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6E03mBAMwsG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "\n",
        "logs_dir = \"logs\"\n",
        "project_name = \"sber-td-course\"\n",
        "exp_name = \"ruBert-base-v2\"\n",
        "\n",
        "\n",
        "os.environ['WANDB_API_KEY'] = open('/content/drive/MyDrive/ssh/wandbkey.txt').read().strip()\n",
        "logger = WandbLogger(\n",
        "    save_dir=logs_dir,\n",
        "    project=project_name,\n",
        "    name=exp_name,\n",
        ")\n",
        "\n",
        "# для простоты будем мониторить лосс и по нему сохранять чекпоинты\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    monitor=\"val_loss\",\n",
        "    dirpath=\"./check_points\",\n",
        "    filename=\"runne_ner-{epoch:02d}-{val_loss:.4f}\",\n",
        "    save_top_k=2,\n",
        "    mode=\"min\",\n",
        ")\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    num_sanity_val_steps=0, \n",
        "    gpus=[0], \n",
        "    max_epochs=5, \n",
        "    logger=logger,\n",
        "    callbacks=[checkpoint_callback]\n",
        ")\n",
        "\n",
        "trainer.fit(m, train_dl, valid_dl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ij3NK5B_cb98"
      },
      "outputs": [],
      "source": [
        "!ls check_points"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87OwXqpxKJ9P"
      },
      "source": [
        "# 4. Инференс модели"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35LnS6_IyrIP"
      },
      "source": [
        "Для инференса нам потребуется:\n",
        "1. Загрузить лучшую (по нашему мнению модель)\n",
        "2. Подготовить тестовый набор, то есть инициализируем **CustomDataset** и **Dataloader** (что уже сделали)\n",
        "3. Определить метод трансформации логитов в метки, а затем восстановление исходных символьных координат\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYRMf-aHt1xh"
      },
      "source": [
        "## 4.1. Загрузка модели из checkpoint\n",
        "\n",
        "Находим лучшую модель из последнего запуска обусения встроенным функционалом checkpoint_callback"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model_path = checkpoint_callback.best_model_path\n",
        "best_model_path"
      ],
      "metadata": {
        "id": "_OFjjrKxuZFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EkAdq5DiQTI8"
      },
      "outputs": [],
      "source": [
        "model = NERModel.load_from_checkpoint(best_model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ура! Наша модель успешно загружена. Всплывающий ворнинг говорит нам только о том, что в процессе загрузки мы сначала загрузили исходные веса `BERT` и в них нех ватило параметров, чтобы проинициализировать классификационную голову, но затем поверх них загрузились результирующие веса затюненой модели. Напомню, что загрузка претренированных весов (без головы) выполнялась этим кодом:\n",
        "\n",
        "`self.model = BertForTokenClassification.from_pretrained('sberbank-ai/ruBert-base', num_labels=num_labels)`\n"
      ],
      "metadata": {
        "id": "CZzhModFvM6t"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlIzgQ9pt1xh"
      },
      "source": [
        "## 4.2. Преобразование сигналов в результат"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfyIWslbt1xh"
      },
      "source": [
        "Осталось научиться преобразовывать логиты в символьные спаны, то есть привести предсказания модели к виду исходной разметки.\n",
        "\n",
        "Для этого нам понадобится две сущности:\n",
        "\n",
        "\n",
        "\n",
        "1.   Экземпляр класса `CustomDataset`. Он содержит в себе информацию о токенах и их позициях в исходном тексте.\n",
        "2.   Логиты модели, чтобы получи соответствующий лейбл для каждого токена.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G17R4PKml9Bc"
      },
      "outputs": [],
      "source": [
        "def transform_logits_to_char_spans(dataset: CustomDataset, logits: torch.tensor):\n",
        "\n",
        "\n",
        "  transfomed_predictions = []\n",
        "\n",
        "  # Проверим что количество предсказаний и количество исходных примеров совпадают\n",
        "  assert len(dataset.tokens_per_sample) == len(logits), \"len(dataset.tokens_per_sample) != len(logits)\"\n",
        "\n",
        "\n",
        "  # Итерируемся по предложению (разбито на токены) и логитам\n",
        "  for sample, markup in zip(dataset.tokens_per_sample, logits):\n",
        "\n",
        "    # Для каждого предложения будем собирать исходный текст из окенов и позиций\n",
        "    text_sample = [\" \"] * sample[-1][1]\n",
        "    # Здесь будут храниться символьные координаты сущностей\n",
        "    char_coords = []\n",
        "\n",
        "    prev_ent = None\n",
        "    \n",
        "    # Итерируемся по токенам и лейблам для того, чтобы перейти от потокенной классификации к символьным спанам\n",
        "    for (s, e, token), label in zip(sample, markup):\n",
        "        text_sample[s:e] = list(token)\n",
        "\n",
        "        # Если предсказанный лейбл == \"O\", обрабатываем две ситуации: \n",
        "        #  - Завершаем создание некоторого спана, то есть сущности\n",
        "        #  - Пропускаем токен\n",
        "        if label == \"O\":\n",
        "          if prev_ent:\n",
        "            char_coords.append(prev_ent)\n",
        "          prev_ent = None\n",
        "          continue\n",
        "\n",
        "        # Если предсказанный лейбл != \"O\", обрабатываем две ситуации: \n",
        "        #  - Обновляем существующий спан, продлеваем его на один токен вправо\n",
        "        #  - Начинаем создавать новый спан\n",
        "        if prev_ent:\n",
        "          if label != prev_ent[-1]:\n",
        "            char_coords.append(prev_ent)\n",
        "            prev_ent = s, e, label\n",
        "            continue\n",
        "          if label == prev_ent[-1]:\n",
        "            prev_ent = prev_ent[0], e, prev_ent[-1]\n",
        "            continue\n",
        "\n",
        "        if prev_ent is None:\n",
        "          prev_ent = s, e, label\n",
        "          continue\n",
        "\n",
        "    # Не забываем обработать последний токен\n",
        "    if prev_ent:\n",
        "      if label != prev_ent[-1]:\n",
        "        char_coords.append(prev_ent)\n",
        "        char_coords.append((s, e, label))\n",
        "      if label == prev_ent[-1]:\n",
        "        char_coords.append((prev_ent[0], e, prev_ent[-1]))\n",
        "\n",
        "    # Склеиваем тексты из токенов, которые мы собирали\n",
        "    transfomed_predictions.append((''.join(text_sample), char_coords)) \n",
        "   \n",
        "  return transfomed_predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_HYGXmZt1xi"
      },
      "source": [
        "## 4.3. Собираем все вместе"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-k_6GXW0sli"
      },
      "outputs": [],
      "source": [
        "def inference_model(\n",
        "    model: nn.Module, \n",
        "    device: str, \n",
        "    dataset: CustomDataset, \n",
        "    dataloader: DataLoader\n",
        "  ) -> List[Tuple[str, list]]:\n",
        "  \"\"\"\n",
        "  Переносим модель на девайс\n",
        "  Выполняем предикт с помощью модели\n",
        "  Преобразуем логиты в символьные спаны\n",
        "  \"\"\"\n",
        "  model = model.to(device)\n",
        "  logits = model.predict(dataloader)\n",
        "  return transform_logits_to_char_spans(dataset, logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AShyl6Y01Rfg"
      },
      "outputs": [],
      "source": [
        "result = inference_model(\n",
        "    model, \n",
        "    \"cuda:0\", \n",
        "    test_ds, \n",
        "    test_dl\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daxZ8znhla9g"
      },
      "outputs": [],
      "source": [
        "for sample_predictions in result[:10]:\n",
        "  show_span_line_markup(*sample_predictions)\n",
        "  print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j30Liiyit1xi"
      },
      "source": [
        "## 4.4. Анализ ошибок \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9-zl1bQ3fAT"
      },
      "source": [
        "Для выполнения анализа ошибок нам потребуется определить еще несколько методов. Мы будем искать несовпадения в спанах и визулизировать только их, таким образом мы сможем понять:\n",
        "1. Есть ли у модели проблемы с предсказанием сущностей из-за разрыва, то есть может ли модель в спане из трех токенов не отнести средний к сущности\n",
        "2. Есть ли у нас over-/under- предикт."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lt3I1CHF1n4T"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "\n",
        "def get_lists_diff(a: list, b: list) -> Tuple[list, list]:\n",
        "    a_counter = Counter(a)\n",
        "    b_counter = Counter(b)\n",
        "    only_a = (a_counter - b_counter).elements()\n",
        "    only_b = (b_counter - a_counter).elements()\n",
        "    return list(only_a), list(only_b)\n",
        "\n",
        "\n",
        "def get_spans_diff(\n",
        "        gold_spans: List[tuple], predicted_spans: List[tuple]\n",
        ") -> List[tuple]:\n",
        "    only_gold, only_predicted = get_lists_diff(gold_spans, predicted_spans)\n",
        "    spans = [(start, end, \"G_\" + type) for start, end, type in only_gold]\n",
        "    spans.extend([(start, end, \"P_\" + type) for start, end, type in only_predicted])\n",
        "    return spans\n",
        "\n",
        "\n",
        "def visualize_difference(target_samples, \n",
        "             predictoons, \n",
        "             n_samples = 10, \n",
        "             ):\n",
        "  for target_markup, predition in list(zip(test, result))[:n_samples]:\n",
        "    sentence_with_coord, gols_entities = target_markup\n",
        "    _, predited_entities = predition\n",
        "    diff = get_spans_diff(gols_entities, predited_entities) \n",
        "    show_span_line_markup(sentence_with_coord[-1], diff)\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_SAMPLES = 10"
      ],
      "metadata": {
        "id": "AttkDReeu4wL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzUvsWJr5Xq8"
      },
      "outputs": [],
      "source": [
        "visualize_difference(test, result, NUM_SAMPLES)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sample_predictions in result[:NUM_SAMPLES]:\n",
        "  show_span_line_markup(*sample_predictions)\n",
        "  print()"
      ],
      "metadata": {
        "id": "wyWrzkcqu3P1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence_info, markups in test[:NUM_SAMPLES]:\n",
        "  show_span_line_markup(sentence_info[-1], markups)\n",
        "  print()"
      ],
      "metadata": {
        "id": "ySkiqEuovJVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Конвертация в jit\n",
        "\n",
        "Изучив ошибки модели и приняв риски, мы переходим к конвертации модели в jit. Для того чтобы нам было проще запустить это папйлайн на инференс в дальнешйем. Вы сталкиваетесь с этим уже не в первый раз, так что эти шаги не должны вызвать у вас затруднений. "
      ],
      "metadata": {
        "id": "zm0qh80btsSt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random \n",
        "\n",
        "bert = model.model\n",
        "bert.eval()\n",
        "\n",
        "max_seq_len = 256\n",
        "\n",
        "tokens_tensor = torch.tensor([[random.randint(0, 33000) for i in range(max_seq_len)]])\n",
        "att_mask_tensors = torch.tensor([[1] * max_seq_len])\n",
        "\n",
        "module = torch.jit.trace(bert.to('cpu'), [tokens_tensor, att_mask_tensors], strict=False)\n",
        "\n",
        "torch.jit.save(module, \"./ner_runne.jit\")"
      ],
      "metadata": {
        "id": "D3yvkdVdtp5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls ./"
      ],
      "metadata": {
        "id": "KUBFUNEowf-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "После того как мы успешно сохранили модель, давайте проверим загрузку и соберем весь код, который нам понадовится для инференс, ведь он у нас написан :)"
      ],
      "metadata": {
        "id": "BrmyFt18wkkY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.jit.load(\"./ner_runne.jit\")"
      ],
      "metadata": {
        "id": "sloHGUSIuKn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выделим в отдельный метод форматирование индексов, полученных с логитов после предказания модель. До этого данный метод существовал в рамках `NERModel`"
      ],
      "metadata": {
        "id": "lOMfBkEMxMlE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index2label = {0: \"O\", 1: 'PERSON', 2:'ORGANIZATION'}\n",
        "\n",
        "def format_predictions(\n",
        "    predictions: torch.tensor, \n",
        "    tokens_mask: torch.tensor, \n",
        "    dictionary: Dict[int, str]\n",
        "  ) -> List[List[str]]:\n",
        "    batch_preds = []\n",
        "    for pred, mask in zip(predictions, tokens_mask):\n",
        "        batch_preds.append(\n",
        "            [index2label[pred[i_m]]\n",
        "             for i_m, mask_value in enumerate(mask)\n",
        "             if mask_value != -100]\n",
        "        )\n",
        "    return batch_preds"
      ],
      "metadata": {
        "id": "4eNjZKhbxLfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Также отчуждаем функцию `predict`. Ее основные дейтсвия:\n",
        "- перевод данных на `device`, где будет производится вычисление\n",
        "- запуск модели на предсказание на батчах\n",
        "- форматирование логитов в индексы лейблов\n",
        "- форматирование индексов в лейблы\n"
      ],
      "metadata": {
        "id": "-DlnufN_2yBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, predict_dl, device, dictionary):\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    with torch.no_grad():\n",
        "        for batch in predict_dl:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            out = model(\n",
        "                input_ids=batch[\"input_ids\"],\n",
        "                attention_mask=batch[\"attention_mask\"]\n",
        "            )\n",
        "            predictions = out[\"logits\"].argmax(dim=-1).cpu().numpy()\n",
        "            preds.extend(\n",
        "                format_predictions(predictions, batch[\"token_start_mask\"], dictionary)\n",
        "            )\n",
        "    return preds"
      ],
      "metadata": {
        "id": "u9Brv-MKuODd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Общий метод для инференса на `jit`:"
      ],
      "metadata": {
        "id": "Wx0gNRzm3biN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def inference_jit_model(model, device, dataset, dataloader, dictionary) -> List[Tuple[str, list]]:\n",
        "    model = model.to(device)\n",
        "    logits = predict(model, dataloader, device, dictionary)\n",
        "    return transform_logits_to_char_spans(dataset, logits)"
      ],
      "metadata": {
        "id": "JNO6WTBu3ZY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Как вы уже могли заметить, мы не копируем `CustomDataset` и `DataLoder`. Однако они нам также понадобятся."
      ],
      "metadata": {
        "id": "jZL8zdRV3mGi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = inference_jit_model(model, \"cuda:0\", test_ds, test_dl, index2label)"
      ],
      "metadata": {
        "id": "6Pny_fpCuQL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ipymarkup import show_span_line_markup\n",
        "\n",
        "for sample_predictions in result[:5]:\n",
        "  show_span_line_markup(*sample_predictions)\n",
        "  print()"
      ],
      "metadata": {
        "id": "M8u6liNAuRlw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### На реальном примере:\n",
        "\n",
        "Возьмем свежую статью, например вот [эту](https://lenta.ru/articles/2022/10/23/king/) - с \"Лента.ру\""
      ],
      "metadata": {
        "id": "D14L1wY36c5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news_sample = \"\"\"Ваше королевское. Карл III взошел на престол в трудное время. Станет ли его правление закатом британской монархии? Смерть королевы Великобритании Елизаветы II стала серьезным потрясением для нации: монарх правила более 70 лет и казалась многим чем-то неизменным, константой в постоянно меняющейся окружающей действительности. После смерти королевы на престол взошел ее старший сын принц Чарльз, принявший имя Карл III. Нового короля ждет весьма непростое правление. В Соединенном Королевстве все отчетливее прослеживаются тенденции к разъединению: Шотландия готовится к уже второму референдуму о независимости, а на выборах в Северной Ирландии победила националистическая партия «Шинн Фейн», выступающая за объединение с Республикой Ирландия. Неспокойно и в других 14 государствах, где британский монарх по-прежнему считается главой государства: там все громче звучат призывы порвать с монархией и объявить себя республиками. На правление Елизаветы II пришелся закат Британской империи. Станет ли правление Карла III закатом британского королевства и монархии вообще — разбиралась «Лента.ру».\"\"\"\n",
        "print(news_sample)"
      ],
      "metadata": {
        "id": "0vr52ctl4K6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "segmented_news_sample = sentence_segmentation([news_sample], return_coords=True) #  Вернем координаты предложений, чтоб если что собрать целый текст (то есть сдвинуть сущности)\n",
        "example_ds = CustomDataset(samples=[(s, []) for s in segmented_news_sample]) #  Подготовим загрузчик данных, эмулируем сущности пустым списком\n",
        "example_dl = DataLoader(example_ds, batch_size=16) #  Собираем все в батчи\n",
        "example_result = inference_jit_model(model, \"cuda:0\", example_ds, example_dl, index2label) #  Инференс модели и конвертация в символьные спаны\n",
        "\n",
        "for sample_predictions in example_result:\n",
        "  show_span_line_markup(*sample_predictions)\n",
        "  print()"
      ],
      "metadata": {
        "id": "-h-SggZh5as_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJOQxCcOt1xj"
      },
      "source": [
        "# 5. Подведение итогов"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70Vjkvddt1xj"
      },
      "source": [
        "Мы научились:\n",
        "- работать с форматом BRAT\n",
        "- анализировать NER разметку\n",
        "- готовить входную последовательность для подачи в модель BERT\n",
        "- формулировать задачу NER как задачу классификации токенов\n",
        "- готовить целевую последовательность для подачи в модель BERT\n",
        "- считать лосс с помощью кросс-энтропии\n",
        "- считать метрики для задачи NER\n",
        "- проводить анализ ошибок\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Домашняя работа"
      ],
      "metadata": {
        "id": "Id4-2YkZhLFr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "В качестве домашнего задания предлагается обучить модель извлечения сущностей, которая станет частью финального сервиса, то есть итогового проекта.\n",
        "Данные находятся в репозитории в архиве `RuRED.tar.gz`. Это набор новостных статей и разметка к ним.\n",
        "Подробнее о том, как и для чего создавалась эта выборка вы можете прочитать в этой [статье](https://www.dialog-21.ru/media/5093/gordeevdiplusetal-031.pdf)"
      ],
      "metadata": {
        "id": "Q24y2JtThX95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/DeepLearning/IntelligentDocumentProcessing/Resources/4_Named_Entity_Recognition"
      ],
      "metadata": {
        "id": "ysZS0piahQRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xf '/content/drive/MyDrive/DeepLearning/IntelligentDocumentProcessing/Resources/4_Named_Entity_Recognition/RuRED.tar.gz'"
      ],
      "metadata": {
        "id": "dWAgpdQlhe3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls ./RuRED-splitted"
      ],
      "metadata": {
        "id": "qPN7Mf4zi5a5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для построения модели извлечения вы можете воспользоваться пайплайном, который мы построили. Однако его нужно адаптировать к этому набору данных.\n",
        "\n",
        "Что нужно сделать?\n",
        "\n",
        "- Считать данные (только NER, Relation extraction не нужен)\n",
        "- Познакомиться с разметкой и удостовериться, что пересекающихся сущностей нет\n",
        "- Определиться с длиной обучающего примера для модели\n",
        "- Проверить распределения по классам токенов на тренировочной, валидационной и тестовой выборок\n",
        "- Обучить модель\n",
        "- Оценить качество на тестовой выборке\n",
        "- Проанализировать ошибики и переобучить (одну или даже две модели) при необходимости"
      ],
      "metadata": {
        "id": "SGbm50KbjpFt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rqhQjHkNjGCI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "B0koq6963n8x",
        "4twyE2Q3wvSj",
        "9_oj7bd6wvSm",
        "Tg-nCxo0wvSq",
        "4vXtrUM_wvSr",
        "rOXNtfQG_wq0",
        "J1eun7iyQ7dN"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "c6db8c5fd8d01916d37fe0e434dd319f9a651ecb2238db7761235e698e9c3eab"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}